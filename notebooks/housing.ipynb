{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from sklearn.compose import (\n",
    "    ColumnTransformer,\n",
    "    make_column_selector,\n",
    "    make_column_transformer,\n",
    ")\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    FunctionTransformer,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    ")\n",
    "\n",
    "# import sklearn kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import set_config\n",
    "from src.helpers import helper_functions, load_data, visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_housing_raw = load_data.load_housing_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "project_path = helper_functions.get_project_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Plan\n",
    "1. splitting train-test\n",
    "2. exploring data\n",
    "3. data preparation pipeline (cleaning, imputing, feature engineering)\n",
    "4. hyperparameter tuning\n",
    "5. overfitting/underfitting check\n",
    "6. evaluation on testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Quick EDA to know how to stratify and split the data into train/test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When splitting our data, we want to make sure the training set is representative of the cases we want to generalize to. Otherwise, we would train machine learning models that would not make accurate predictions.\n",
    "That is why we need to make sure the distribution of key features correlated to our target are preserved in the test set. By doing so, we are evaluating our machine learning models against representative data and hence, we can trust the quality of our models' predictions.\n",
    "\n",
    "Splitting the data in this manner is called _stratified sampling_. To do so, we need to do some basic exploratory of our data. This is what we will do now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_housing_raw.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot a correlation matrix using seaborn and coolwarm colormap\n",
    "sns.heatmap(df_housing_raw.corr(), square=True, annot=True, cmap=\"coolwarm\")\n",
    "# add title\n",
    "plt.title(\"Correlation Matrix\")\n",
    "# save the plot and make sure the axis labels are not cut off\n",
    "plt.savefig(project_path / \"images\" / \"correlation_matrix.png\", bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 insights from the correlation plot:\n",
    "- the median_house_value (target) is quite correlated to the median_income, so we will use it to split the data (in a stratified manner)\n",
    "- total_bedrooms has 207 missing values and is very correlated to households, so we will use it to fill the missing values with a customer sklearn transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of all the numerical features, add a title to the plot and save the plot to the images folder\n",
    "df_housing_raw.hist(bins=50, figsize=(20, 10))\n",
    "plt.suptitle(\"Distribution of numerical features\", fontsize=16)\n",
    "plt.savefig(project_path / \"images\" / \"numerical_features_distribution.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_housing = df_housing_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# bin median_income into 5 bins ([0, 1.5, 3, 4.5, 6, np.inf]) and plot the value counts\n",
    "df_housing[\"median_income_bin\"] = pd.cut(\n",
    "    df_housing[\"median_income\"],\n",
    "    bins=[0, 1.5, 3, 4.5, 6, np.inf],\n",
    "    labels=[1, 2, 3, 4, 5],\n",
    ")\n",
    "df_housing[\"median_income_bin\"].value_counts().sort_index().plot(kind=\"bar\")\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# show each value on top of each bar (centered)\n",
    "for index, value in enumerate(\n",
    "    df_housing[\"median_income_bin\"].value_counts().sort_index()\n",
    "):\n",
    "    plt.text(index, value, str(value), ha=\"center\")\n",
    "\n",
    "plt.title(\"Median income category\")\n",
    "plt.xlabel(\"Median income category\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Split into train/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_housing[\"median_income_bin\"] = pd.cut(\n",
    "    df_housing[\"median_income\"],\n",
    "    bins=[0, 1.5, 3, 4.5, 6, np.inf],\n",
    "    labels=[1, 2, 3, 4, 5],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(\n",
    "    df_housing,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_housing[\"median_income_bin\"],\n",
    ")\n",
    "\n",
    "df_train.drop(\"median_income_bin\", axis=1, inplace=True)\n",
    "df_test.drop(\"median_income_bin\", axis=1, inplace=True)\n",
    "\n",
    "# split into X and y\n",
    "X_train = df_train.drop(\"median_house_value\", axis=1)\n",
    "y_train = df_train[\"median_house_value\"].copy()\n",
    "\n",
    "X_test = df_test.drop(\"median_house_value\", axis=1)\n",
    "y_test = df_test[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# More EDA (training set only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df_train.plot(\n",
    "    kind=\"scatter\",\n",
    "    x=\"longitude\",\n",
    "    y=\"latitude\",\n",
    "    s=df_train[\"population\"] / 50,\n",
    "    c=\"median_house_value\",\n",
    "    cmap=\"jet\",\n",
    "    ax=ax,\n",
    "    alpha=0.5,\n",
    "    title=\"median_house_value geospatial distribution\",\n",
    ")\n",
    "plt.savefig(project_path / \"images/median_house_value_geospatial.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df_train.plot(\n",
    "    kind=\"scatter\",\n",
    "    x=\"median_income\",\n",
    "    y=\"median_house_value\",\n",
    "    alpha=0.5,\n",
    "    title=\"median_house_value in relation to median_income\",\n",
    "    ax=ax,\n",
    ")\n",
    "plt.savefig(project_path / \"images/house_value_vs_income.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plot the value counts of ocean_proximity\n",
    "df_train[\"ocean_proximity\"].value_counts().plot(kind=\"bar\")\n",
    "plt.xticks(rotation=0)\n",
    "# add the value on top of each bar (centered) and add a title\n",
    "for index, value in enumerate(df_train[\"ocean_proximity\"].value_counts()):\n",
    "    plt.text(index, value, str(value), ha=\"center\")\n",
    "plt.title(\"Ocean proximity\")\n",
    "plt.xlabel(\"Ocean proximity\")\n",
    "plt.ylabel(\"Count\")\n",
    "# save the plot to the images folder\n",
    "plt.savefig(project_path / \"images/ocean_proximity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of households with annotation of values for each bin\n",
    "# make sure the axis labels are not cut off\n",
    "fig, ax = plt.subplots()\n",
    "pd.cut(\n",
    "    df_housing[\"households\"],\n",
    "    bins=[\n",
    "        0,\n",
    "        100,\n",
    "        200,\n",
    "        300,\n",
    "        400,\n",
    "        500,\n",
    "        600,\n",
    "        700,\n",
    "        800,\n",
    "        900,\n",
    "        1000,\n",
    "        1100,\n",
    "        1200,\n",
    "        1300,\n",
    "        np.inf,\n",
    "    ],\n",
    ").value_counts().sort_index().plot(kind=\"bar\", ax=ax)\n",
    "ax.set_title(\"Distribution of bins of 'households' feature\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "for index, value in enumerate(\n",
    "    pd.cut(\n",
    "        df_housing[\"households\"],\n",
    "        bins=[\n",
    "            0,\n",
    "            100,\n",
    "            200,\n",
    "            300,\n",
    "            400,\n",
    "            500,\n",
    "            600,\n",
    "            700,\n",
    "            800,\n",
    "            900,\n",
    "            1000,\n",
    "            1100,\n",
    "            1200,\n",
    "            1300,\n",
    "            np.inf,\n",
    "        ],\n",
    "    )\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    "):\n",
    "    plt.text(index, value, str(value), ha=\"center\")\n",
    "plt.savefig(project_path / \"images/households_distribution.png\" , bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column with adequate bin households\n",
    "df_housing[\"households_bin\"] = pd.cut(\n",
    "    df_housing[\"households\"],\n",
    "    bins=[0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, np.inf],\n",
    "    labels=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot total_bedrooms grouped by households_bin\n",
    "df_housing.groupby(\"households_bin\")[\"total_bedrooms\"].mean().plot(kind=\"bar\")\n",
    "# add the value (rounded nearest integer) on top of each bar (centered) and add a title\n",
    "for index, value in enumerate(\n",
    "    df_housing.groupby(\"households_bin\")[\"total_bedrooms\"].mean()\n",
    "):\n",
    "    plt.text(index, value, str(round(value)), ha=\"center\")\n",
    "\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.title(\"Average total_bedrooms per households_bin\")\n",
    "plt.xlabel(\"households_bin\")\n",
    "plt.ylabel(\"Average total_bedrooms\")\n",
    "# save the plot to the images folder\n",
    "plt.savefig(project_path / \"images/total_bedrooms_per_households_bin.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's create a customer imputer for total_bedrooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom imputer that fill missing values of a column with its median or mean by groups of by\"\"\"\n",
    "\n",
    "    def __init__(self, variable, by):\n",
    "        self.variable = variable\n",
    "        self.by = by\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X[self.variable] = X.groupby(self.by)[self.variable].transform(\n",
    "            lambda x: x.fillna(x.median())\n",
    "        )\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer_test = GroupedImputer(variable=\"total_bedrooms\", by=\"households_bin\")\n",
    "# df_housing = imputer_test.fit_transform(df_housing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some feature engineering with the location (lat and long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom transformer in scikit-learn that fits a Kmeans using the lat and long features and adds the distances to the clusters as new features\n",
    "class KMeansTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom transformer that fits a Kmeans using the lat and long features and adds the distances to the clusters as new features\"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters=5, weights=None, pass_through=False):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.weights = weights\n",
    "        self.pass_through = pass_through\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.pass_through:\n",
    "            return self\n",
    "        self.kmeans = KMeans(n_clusters=self.n_clusters)\n",
    "        self.kmeans.fit(X[[\"latitude\", \"longitude\"]], sample_weight=self.weights)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.pass_through:\n",
    "            return X\n",
    "\n",
    "        X[\"kmeans_cluster\"] = self.kmeans.predict(X[[\"latitude\", \"longitude\"]])\n",
    "        for i in range(self.n_clusters):\n",
    "            X[f\"kmeans_distance_{i}\"] = self.kmeans.transform(\n",
    "                X[[\"latitude\", \"longitude\"]]\n",
    "            ).iloc[:, i]\n",
    "        return X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the clusters look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform the kmeans transformer on X_train\n",
    "kmeans_transformer = KMeansTransformer(n_clusters=10, weights=y_train)\n",
    "X_train = kmeans_transformer.fit_transform(X_train)\n",
    "\n",
    "# plot the kmeans clusters and draw lines to the centroids for each cluster for each row.\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "X_train.plot(\n",
    "    kind=\"scatter\",\n",
    "    x=\"longitude\",\n",
    "    y=\"latitude\",\n",
    "    c=\"kmeans_cluster\",\n",
    "    cmap=\"jet\",\n",
    "    alpha=0.5,\n",
    "    ax=ax,\n",
    ")\n",
    "for i in range(kmeans_transformer.n_clusters):\n",
    "    plt.plot(\n",
    "        kmeans_transformer.kmeans.cluster_centers_[i, 1],\n",
    "        kmeans_transformer.kmeans.cluster_centers_[i, 0],\n",
    "        \"kx\",\n",
    "    )\n",
    "    plt.text(\n",
    "        kmeans_transformer.kmeans.cluster_centers_[i, 1] + 0.01,\n",
    "        kmeans_transformer.kmeans.cluster_centers_[i, 0] + 0.01,\n",
    "        f\"Cluster {i}\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "# add a title\n",
    "plt.title(\"Kmeans clusters\")\n",
    "# save the plot to the images folder\n",
    "plt.savefig(project_path / \"images/kmeans_clusters.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revert X_train to before KmeansTransformer was applied\n",
    "X_train = X_train.drop(\n",
    "    columns=[f\"kmeans_distance_{i}\" for i in range(kmeans_transformer.n_clusters)]\n",
    "    + [\"kmeans_cluster\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline for the numerical and categorical features and add the KmeansTransformer to the numerical pipeline\n",
    "num_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"kmeans\", KMeansTransformer(n_clusters=10, weights=y_train)),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "cat_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# create a column transformer that applies the numerical and categorical pipeline to the numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", num_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# create a pipeline that applies the preprocessor and a random forest regressor\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"regressor\", RandomForestRegressor()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# show the pipeline\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice the pipeline to only the num preprocessor\n",
    "num_pipeline = pipeline.steps[0][1].transformers[0][1]\n",
    "# remove (in place) the kmeans transformer from the num pipeline\n",
    "# num_pipeline.steps.pop(1)\n",
    "\n",
    "# column transformer that only applies the num pipeline to the numerical features\n",
    "num_pipeline = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", num_pipeline, make_column_selector(dtype_include=np.number)),  \n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# fit X_train using num_pipeline\n",
    "num_pipeline.fit_transform(X_train, y_train)\n",
    "\n",
    "# num_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer = SimpleImputer(strategy=\"median\")\n",
    "# kmeans = KMeansTransformer(n_clusters=10, weights=y_train)\n",
    "# X_train[\"total_bedrooms\"] = imputer.fit_transform(X_train[[\"total_bedrooms\"]], y_train)\n",
    "# display(X_train)\n",
    "# X_train = kmeans.fit_transform(X_train, y_train)\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert a custom debugger transformer in the pipeline to inspect the data at each step\n",
    "class Debugger(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom transformer that prints the shape of the data at each step\"\"\"\n",
    "\n",
    "    def __init__(self, name=\"\"):\n",
    "        self.name = name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print(f\"{self.name} transform: {X.shape}\")\n",
    "        return X\n",
    "\n",
    "# insert the debugger transformer in the pipeline just after num SimpleImputer\n",
    "pipeline.steps[0][1].transformers[0][1].steps.insert(\n",
    "    1, (\"debugger\", Debugger(name=\"num imputer\"))\n",
    ")\n",
    "# insert the debugger transformer in the pipeline just after num KmeansTransformer\n",
    "pipeline.steps[0][1].transformers[0][1].steps.insert(\n",
    "    3, (\"debugger2\", Debugger(name=\"num kmeans\"))\n",
    ")\n",
    "# insert the debugger transformer in the pipeline just after num StandardScaler\n",
    "pipeline.steps[0][1].transformers[0][1].steps.insert(\n",
    "    5, (\"debugger3\", Debugger(name=\"num scaler\"))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Pipeline(pipeline.steps[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the pipeline on X_train and y_train\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# show the score of the pipeline on X_train and y_train and X_test and y_test\n",
    "print(f\"Train score: {pipeline.score(X_train, y_train):.3f}\")\n",
    "print(f\"Test score: {pipeline.score(X_test, y_test):.3f}\")\n",
    "\n",
    "\n",
    "# create a dataframe with the feature importances\n",
    "df_feature_importances = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": pipeline.named_steps[\"preprocessor\"]\n",
    "        .transformers_[0][1][\"kmeans\"]\n",
    "        .kmeans.cluster_centers_.flatten(),\n",
    "        \"importance\": pipeline.named_steps[\"regressor\"].feature_importances_,\n",
    "    }\n",
    ")\n",
    "\n",
    "# plot the feature importances\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "df_feature_importances.plot(kind=\"bar\", x=\"feature\", y=\"importance\", ax=ax)\n",
    "# add a title\n",
    "plt.title(\"Feature importances\")\n",
    "# save the plot to the images folder\n",
    "plt.savefig(project_path / \"images/feature_importances.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# create a dataframe with the predictions on X_train\n",
    "df_predictions = pd.DataFrame(\n",
    "    {\n",
    "        \"prediction\": pipeline.predict(X_train),\n",
    "        \"actual\": y_train,\n",
    "    }\n",
    ")\n",
    "\n",
    "# plot the predictions on X_train\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "df_predictions.plot(kind=\"scatter\", x=\"prediction\", y=\"actual\", ax=ax)\n",
    "# add a title\n",
    "plt.title(\"Predictions on X_train\")\n",
    "# save the plot to the images folder\n",
    "plt.savefig(project_path / \"images/predictions_on_X_train.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# create a dataframe with the residuals on X_train\n",
    "df_residuals = pd.DataFrame(\n",
    "    {\n",
    "        \"residual\": pipeline.predict(X_train) - y_train,\n",
    "        \"actual\": y_train,\n",
    "    }\n",
    ")\n",
    "\n",
    "# plot the residuals on X_train\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "df_residuals.plot(kind=\"scatter\", x=\"residual\", y=\"actual\", ax=ax)\n",
    "# add a title\n",
    "plt.title(\"Residuals on X_train\")\n",
    "# save the plot to the images folder\n",
    "plt.savefig(project_path / \"images/residuals_on_X_train.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters grid to search for the best hyperparameters for the random forest regressor.\n",
    "# switch on and off the KmeansTransformer by setting pass_through to True or False\n",
    "\n",
    "param_grid_preprocessor = {\n",
    "    # imputer: either SimpleImputer or GroupedImputer or KNNImputer\n",
    "    \"preprocessor__num__imputer\": [\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        GroupedImputer(variable=\"total_bedrooms\", by=\"households_bin\"),\n",
    "        KNNImputer(),\n",
    "    ],\n",
    "    # scaler: either StandardScaler or MinMaxScaler or RobustScaler\n",
    "    \"preprocessor__num__scaler\": [StandardScaler(), MinMaxScaler(), RobustScaler()],\n",
    "}\n",
    "\n",
    "param_grid_kmeans = {\n",
    "    \"preprocessor__num__kmeans__n_clusters\": [5, 10],\n",
    "    \"preprocessor__num__kmeans__weights\": [None, y_train],\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    \"regressor__n_estimators\": [100, 200, 300],\n",
    "    \"regressor__max_depth\": [None, 5, 10],\n",
    "    \"regressor__min_samples_split\": [2, 5, 10],\n",
    "    \"regressor__min_samples_leaf\": [1, 2, 4],\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_full = [\n",
    "    {\n",
    "        # do not use the KmeansTransformer\n",
    "        \"preprocessor__num__kmeans__pass_through\": True,\n",
    "        **param_grid_preprocessor,\n",
    "        **param_grid_rf,\n",
    "    },\n",
    "    # use the KmeansTransformer\n",
    "    {\n",
    "        \"preprocessor__num__kmeans__pass_through\": False,\n",
    "        **param_grid_preprocessor,\n",
    "        **param_grid_kmeans,\n",
    "        **param_grid_rf,\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# create a grid search with 5-fold cross validation\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid_full,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# fit the grid search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the results in a dataframe (drop useless columns)\n",
    "results = pd.DataFrame(grid_search.cv_results_).drop(\n",
    "    columns=[\n",
    "        \"mean_fit_time\",\n",
    "        \"std_fit_time\",\n",
    "        \"mean_score_time\",\n",
    "        \"std_score_time\",\n",
    "        \"params\",\n",
    "        \"split0_test_score\",\n",
    "        \"split1_test_score\",\n",
    "        \"split2_test_score\",\n",
    "        \"split3_test_score\",\n",
    "        \"split4_test_score\",\n",
    "        \"rank_test_score\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# show the results\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save the results to a excel file in models\\tuning_results\n",
    "def save_results(results, filename):\n",
    "    results.to_excel(\n",
    "        project_path / f\"models/tuning_results/{filename}.xlsx\", index=False\n",
    "    )\n",
    "\n",
    "\n",
    "# save the results\n",
    "save_results(results, \"second_tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid_search.cv_results_).sort_values(\"rank_test_score\")\n",
    "results = results[\n",
    "    [\n",
    "        \"param_model\",\n",
    "        \"param_preprocessing__cat__onehotencoder\",\n",
    "        \"param_preprocessing__num__simpleimputer\",\n",
    "        \"param_preprocessing__num__standardscaler\",\n",
    "        \"mean_test_score\",\n",
    "        \"std_test_score\",\n",
    "        \"rank_test_score\",\n",
    "    ]\n",
    "]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test = df_test.drop(\"median_house_value\", axis=1)\n",
    "y_test = df_test[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final_predictions = final_model.predict(X_test)\n",
    "final_rmse = mean_squared_error(y_test, final_predictions, squared=False)\n",
    "final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results.to_excel(\n",
    "    project_path / \"models\" / \"tuning_results\" / \"first_tuning\" / \"first_tuning.xlsx\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_model = joblib.dump(\n",
    "    grid_search.best_estimator_,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homl3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "790e7aa706a45ff75c56b9bc7b8df4c178eff5e56969134269a30c22338dc3f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
